{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adb1a423-9dee-481b-8ce2-fe43dc39c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "SEED=42\n",
    "torch.manual_seed(SEED)\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f09b84e0-c458-4235-b307-2fb4825dab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/home/karthiktiwari/Documents/Projects/sarcasm_detection/Sarcasm_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08054cf1-379c-409c-97a6-fbba9d9fb6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47930, 49597)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[dataset['sarcastic']==1]) , len(dataset[dataset['sarcastic']==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37733f06-e9dd-4560-8d29-bf160072d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=0)\n",
    "for train_index, test_index in sss.split(dataset.loc[:, 'tweet_id'], dataset.loc[:, 'sarcastic']):\n",
    "    train_df = dataset.iloc[train_index,:]\n",
    "    test_df = dataset.iloc[test_index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f80e1282-3ac2-4d0b-9502-dc9665d6aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/karthiktiwari/Data/Sarcasm Detection/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d82a2cf8-b32c-4948-8133-52ec04de32a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# err_ids = []\n",
    "# with open('../err_ids.txt') as file:\n",
    "#     err_ids = file.readlines()\n",
    "# err_ids = [int(idx) for idx in err_ids]\n",
    "\n",
    "# err_i = []\n",
    "# for i in tqdm(range(len(dataset))):\n",
    "#     if dataset.loc[i,'tweet_id'] in err_ids:\n",
    "#         err_i.append(i)\n",
    "\n",
    "# dataset.drop(index = err_i, axis = 0, inplace=True)\n",
    "# dataset.to_csv('/home/karthiktiwari/Documents/Projects/sarcasm_detection/Sarcasm_dataset.csv', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97387f48-1acf-43a2-b4f4-e65a2232c19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to build vocabulary for mapping\"\"\"\n",
    "    def __init__(self, token_to_idx = None, add_unk = True, unk_token = \"<UNK>\", mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",end_seq_token=\"<END>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx: Initialize token to idx dictionary.\n",
    "            add_unk: Whether to include the unknown token in the vocabulary\n",
    "            unk_token: How the unknown token is represented in the vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "            \n",
    "        self._token_to_idx = token_to_idx\n",
    "        \n",
    "        self._idx_to_token = {idx:token \n",
    "                             for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "#         self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "#         self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        \n",
    "    def from_serializable(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            A dictionary that can be serialized\n",
    "        \"\"\"\n",
    "\n",
    "        return cls(**contents)\n",
    "        \n",
    "    def add_token(self, token):\n",
    "        \"\"\" Update mapping dictionaries given the token\n",
    "        Args:\n",
    "            token (str): Token to add to the vocabulary\n",
    "        Returns:\n",
    "            index (int): The index corresponding to the token          \n",
    "        \"\"\"\n",
    "\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "\n",
    "        return index\n",
    "        \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\" Retrieves the index associated with the token\n",
    "            or the UNK index if the token isn't present in the vocabulary\n",
    "\n",
    "        Args:\n",
    "            token (str): The token for which the index has to be retrieved\n",
    "        Returns:\n",
    "            index (int): The index associated with the token in the dictionary\n",
    "\n",
    "        Note: \n",
    "            'UNK Index' has to be >=0 for the UNK functionality\n",
    "        \"\"\"\n",
    "\n",
    "        if self._add_unk:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx.get(token)\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Retrieve the token associated with the index\n",
    "        Args:\n",
    "            index (int): The index to look up\n",
    "        Returns:\n",
    "            token (str): The token associated with the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the vocabulary\n",
    "        \"\"\"\n",
    "\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index %d is not in the vocabulary\" % index)\n",
    "        else:\n",
    "            return self._idx_to_token[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of the vocabulary\n",
    "        \"\"\"\n",
    "        return len(self._token_to_idx)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fb1fe07-27e8-4c7d-857d-ccec64d26ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to build vocabulary for mapping\"\"\"\n",
    "    def __init__(self, token_to_idx = None, add_unk = True, unk_token = \"<UNK>\", mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",end_seq_token=\"<END>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx: Initialize token to idx dictionary.\n",
    "            add_unk: Whether to include the unknown token in the vocabulary\n",
    "            unk_token: How the unknown token is represented in the vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "            \n",
    "        self._token_to_idx = token_to_idx\n",
    "        \n",
    "        self._idx_to_token = {idx:token \n",
    "                             for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "#         self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "#         self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        \n",
    "    def from_serializable(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            A dictionary that can be serialized\n",
    "        \"\"\"\n",
    "\n",
    "        return cls(**contents)\n",
    "        \n",
    "    def add_token(self, token):\n",
    "        \"\"\" Update mapping dictionaries given the token\n",
    "        Args:\n",
    "            token (str): Token to add to the vocabulary\n",
    "        Returns:\n",
    "            index (int): The index corresponding to the token          \n",
    "        \"\"\"\n",
    "\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "\n",
    "        return index\n",
    "        \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\" Retrieves the index associated with the token\n",
    "            or the UNK index if the token isn't present in the vocabulary\n",
    "\n",
    "        Args:\n",
    "            token (str): The token for which the index has to be retrieved\n",
    "        Returns:\n",
    "            index (int): The index associated with the token in the dictionary\n",
    "\n",
    "        Note: \n",
    "            'UNK Index' has to be >=0 for the UNK functionality\n",
    "        \"\"\"\n",
    "\n",
    "        if self._add_unk:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx.get(token)\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Retrieve the token associated with the index\n",
    "        Args:\n",
    "            index (int): The index to look up\n",
    "        Returns:\n",
    "            token (str): The token associated with the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the vocabulary\n",
    "        \"\"\"\n",
    "\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index %d is not in the vocabulary\" % index)\n",
    "        else:\n",
    "            return self._idx_to_token[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of the vocabulary\n",
    "        \"\"\"\n",
    "        return len(self._token_to_idx)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aed79a21-82c1-45c9-84bf-ac71eb97436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sarcastic_tweets(Dataset):\n",
    "    def __init__(self, df, nlp, vocab):\n",
    "        \"\"\"Initializing\n",
    "        Args:\n",
    "            df (Pandas DataFrame): Dataframe consisting of tweets and labels\n",
    "            nlp (spacy object): For preprocessing\n",
    "            vocab (Vocabulary Object): To vectorize the tweets\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.nlp = nlp\n",
    "        self.vocab = vocab\n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        # self._max_seq_length = max(map(measure_len, self.df.tweet_id)) + 1\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        with open(os.path.join(root_dir, str(self.df.tweet_id.iloc[idx])+'.txt')) as file:\n",
    "              tweet = file.read()\n",
    "        \n",
    "        label = self.df.sarcastic.iloc[idx]\n",
    "        return {'tweet':torch.LongTensor(self.preprocess(tweet)), 'label':label}\n",
    "        # return {'tweet':self.preprocess(tweet), 'label':label}\n",
    "    \n",
    "    def preprocess(self, sent):\n",
    "        \n",
    "        #Preprocessing and tokenizing\n",
    "        sent = re.sub(r'RT','',sent)\n",
    "        sent = re.sub('@[a-zA-z0-9_]+','',sent)\n",
    "        sent = re.sub(r'http://[^\\s<>\"]+|www\\.[^\\s<>\"]+','',sent)\n",
    "        sent = re.sub(r'&#[0-9]+','',sent)\n",
    "        sent = re.sub(r'[0-9]+','',sent)\n",
    "        sent = re.sub(r'[^\\w\\s]+', '',sent)\n",
    "        sent = sent.strip()\n",
    "        sent = re.sub('[^a-zA-Z]',' ',sent)\n",
    "        sent = \" \".join(sent.split())\n",
    "        sent = [token.lemma_ for token in nlp(sent) if token.text not in STOP_WORDS]\n",
    "        sent = self.vectorize(sent)\n",
    "        return sent\n",
    "\n",
    "    def vectorize(self, sent):\n",
    "        \"\"\"Converts raw text to numeric vectors using the vocabulary\n",
    "        Args:\n",
    "            sent (str): The tweet to be vectorized\n",
    "        Returns:\n",
    "            vector (list): The vector associated with the tweet\n",
    "        \"\"\"\n",
    "        vector = [self.vocab.begin_seq_index]\n",
    "#         vector = []\n",
    "        for token in sent:\n",
    "            vector.append(vocab.lookup_token(token))\n",
    "        vector.append(self.vocab.end_seq_index)\n",
    "            \n",
    "        return vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "754bc2d2-95bf-44d5-ae90-083f81a1a98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary(token_to_idx=None, add_unk=True)\n",
    "nlp = spacy.load(name='en_core_web_sm')\n",
    "\n",
    "def clean(idx):\n",
    "    with open(os.path.join(root_dir, str(idx)+'.txt')) as tweet:\n",
    "        t = tweet.read()\n",
    "    t = re.sub(r'RT','',t)\n",
    "    t = re.sub('@[a-zA-z0-9_]+','',t)\n",
    "    t = re.sub(r'http://[^\\s<>\"]+|www\\.[^\\s<>\"]+','',t)\n",
    "    t = re.sub(r'&#[0-9]+','',t)\n",
    "    t = re.sub(r'[0-9]+','',t)\n",
    "    t = re.sub(r'[^\\w\\s]+', '',t)\n",
    "    t = t.strip()\n",
    "    t = \" \".join(t.split())\n",
    "    t = [token.lemma_ for token in nlp(t) if token.text not in STOP_WORDS]\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6ecc44f-28d3-4eed-9b18-3a795a8d6a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm.pandas()\n",
    "# cleaned_tweets = train_data.tweet_id.progress_apply(clean)\n",
    "text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89289fe8-8e46-4ba5-96d9-12efa553db0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1036166/1036166 [00:00<00:00, 3078118.28it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('cleaned_tweets.txt', 'r') as  txt:\n",
    "    cleaned_tweets = txt.readlines()\n",
    "cleaned_tweets = cleaned_tweets[0].split(\" \")\n",
    "for i in tqdm(range(len(cleaned_tweets))):\n",
    "    word = cleaned_tweets[i]\n",
    "    text.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12022eb6-d3b0-410d-b91f-55b84a7666bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('cleaned_tweets.txt', 'w') as txt:\n",
    "#     txt.writelines([\" \".join(tweet) for tweet in list(cleaned_tweets)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22628320-0e02-499d-82f3-bc908deb0c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_tweets = list(cleaned_tweets.copy())\n",
    "# for i in tqdm(range(len(cleaned_tweets))):\n",
    "#     for word in cleaned_tweets[i]:\n",
    "#         text.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "051686e1-ab05-43e6-b7fb-14fbfe8cbeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158469/158469 [00:00<00:00, 1437354.38it/s]\n"
     ]
    }
   ],
   "source": [
    "len(text)\n",
    "from collections import Counter\n",
    "count_dict = Counter(text).most_common(len(set(text))-5000)\n",
    "for tup in tqdm(count_dict):\n",
    "    vocab.add_token(tup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da16adf3-da4b-4a3e-aa01-e67025acba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Sarcastic_tweets(train_df,nlp, vocab)\n",
    "test_data = Sarcastic_tweets(test_df, nlp, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a86be054-2d5d-4bd0-8e34-e5d2c9dbc056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tweet': tensor([     1,   2218,      3,  16399,    126,   9409,  11028,      0,      7,\n",
       "           3025,   8938,      3,     25,    642,  11456, 113220,     12,     16,\n",
       "              2]),\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e8cf64b-2d38-4d7a-bfc1-379a7e764241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5085446715428259, 0.5085614682661745)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df[train_df.sarcastic==0]) / len(train_df), len(test_df[test_df.sarcastic==0]) / len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97314aa1-ae6f-49ea-86c3-aa1cfe94f872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(filepath):\n",
    "    \"\"\"Loads the glove embeddings\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): path to the glove embeddings file\n",
    "    Return:\n",
    "        word_to_index (dict): Mappings from word to index\n",
    "        embeddings (np.array): Embeddings of the words in the vocabulary\n",
    "    \"\"\"\n",
    "    word_to_index = {}\n",
    "    embeddings = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as fp:\n",
    "        for index, line in enumerate(fp):\n",
    "            line = line.split(\" \")\n",
    "            word_to_index[line[0]] = index\n",
    "            embedding_i = np.array([float(val) for val in line[1:]])\n",
    "            embeddings.append(embedding_i)\n",
    "    \n",
    "    return word_to_index, np.array(embeddings)\n",
    "\n",
    "def make_embedding_matrix(filepath, words):\n",
    "    \"\"\"Create embedding matrix for a specific set of words\n",
    "    Args:\n",
    "        word_to_index (dict) : mapping of word to index\n",
    "        embeddings (list): embeddings of words\n",
    "        words (list): List of words in the dictionary\n",
    "    Returns:\n",
    "        final_embeddings (np..array) : embedding matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    word_to_idx, embeddings = load_glove(filepath)\n",
    "    embedding_size = embeddings.shape[1]\n",
    "    final_embeddings = np.zeros((len(words), embedding_size))\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_idx:\n",
    "            final_embeddings[i, :] = embeddings[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.zeros(embedding_size)\n",
    "            final_embeddings[i, :] = embedding_i\n",
    "            \n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25c19b19-8a5e-48c3-be0a-9b1e2b8e9604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158472/158472 [00:00<00:00, 1517817.82it/s]\n"
     ]
    }
   ],
   "source": [
    "words=[]\n",
    "for idx in tqdm(range(0, vocab.__len__())):\n",
    "    words.append(vocab.lookup_index(idx))\n",
    "embs = make_embedding_matrix('/home/karthiktiwari/Data/glove.6B/glove.6B.50d.txt', words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38ee1f37-247b-4c1f-a02b-6578a80d917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, label_size, batch_size, embedding_weights, num_layers = 1,bidirectional = False):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(embedding_weights, freeze=False, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size = self.embedding_dim, hidden_size = hidden_dim,\n",
    "                            num_layers = num_layers, bidirectional = bidirectional,batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, label_size)\n",
    " \n",
    "    def forward(self, sentences, train = True):\n",
    "        embeds = self.word_embeddings(sentences)\n",
    "        packed_outputs, _ = self.lstm(embeds)\n",
    "        outputs = self.fc(packed_outputs[:,-1,:].reshape(self.batch_size,-1))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6cd7208",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, label_size, batch_size, embedding_weights, num_layers = 1,bidirectional = False):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=self.embedding_dim, num_heads=1)\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(embedding_weights, freeze=False, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size = self.embedding_dim, hidden_size = hidden_dim,\n",
    "                            num_layers = num_layers, bidirectional = bidirectional,batch_first=True)\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(hidden_dim*2, label_size)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, label_size)\n",
    " \n",
    "    def forward(self, sentences, train = True):\n",
    "        embeds = self.word_embeddings(sentences)\n",
    "        embeds = embeds.permute(0,1,2)\n",
    "        attn_output, _ = self.attention(embeds, embeds, embeds)\n",
    "        packed_outputs, (hidden,cell) = self.lstm(attn_output.permute(0,1,2))\n",
    "        dense_outputs = self.fc(packed_outputs[:,-1,:])\n",
    "        outputs = dense_outputs\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3cb4aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnBasedLSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, label_size, batch_size, embedding_weights, num_layers = 1,bidirectional = False):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=self.embedding_dim, num_heads=1)\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(embedding_weights, freeze=False, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size = self.embedding_dim, hidden_size = hidden_dim,\n",
    "                            num_layers = num_layers, bidirectional = bidirectional,batch_first=True)\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(hidden_dim*2, label_size)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, label_size)\n",
    " \n",
    "    def forward(self, sentences, train = True):\n",
    "        embeds = self.word_embeddings(sentences)\n",
    "        embeds = embeds.permute(0,1,2)\n",
    "        attn_output, _ = self.attention(embeds, embeds, embeds)\n",
    "        packed_outputs, (hidden,cell) = self.lstm(attn_output.permute(0,1,2))\n",
    "        dense_outputs = self.fc(packed_outputs[:,-1,:])\n",
    "        outputs = dense_outputs\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90320c65-e765-4aa9-a2e3-c8a6fcac303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "nlabel = 2\n",
    "num_layers = 2\n",
    "hidden_dim = 64\n",
    "EMBEDDING_DIM = embs.shape[1]\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = LSTMClassifier(embedding_dim=EMBEDDING_DIM,hidden_dim=hidden_dim,label_size=nlabel, batch_size=BATCH_SIZE, embedding_weights=torch.from_numpy(embs).float(), num_layers=num_layers, bidirectional = False)\n",
    "# model = BidirectionalLSTM(embedding_dim=EMBEDDING_DIM,hidden_dim=hidden_dim,label_size=nlabel, batch_size=BATCH_SIZE, embedding_weights=torch.from_numpy(embs).float(), num_layers=num_layers, bidirectional = True)\n",
    "model = model.to(device)\n",
    " \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.8)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    " \n",
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch\n",
    "    \"\"\"\n",
    "    top_pred = preds.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52d8eae5-05a4-4adf-804d-3ae13a2dda1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rewrite collate_ FN function, whose input is the sample data of a batch\n",
    "def collate_fn(batch):\n",
    "\t#Because token_ List is a variable length data, so you need to use a list to load the token of the batch_ list\n",
    "    token_lists = [item['tweet'] for item in batch]\n",
    "    #Each label is an int. we take out all the labels in the batch and reassemble them\n",
    "    labels = [item['label'] for item in batch]\n",
    "    #Converting labels to tensor\n",
    "    labels = torch.LongTensor(labels)\n",
    "    return {\n",
    "    'token_list': torch.nn.utils.rnn.pad_sequence(token_lists, batch_first=True),\n",
    "    'label': labels,\n",
    "    }\n",
    "\n",
    "#When using dataloader to load data, pay attention to collate_ The FN parameter passes in an overridden function\n",
    "trainset = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate_fn, drop_last = True)\n",
    "testset = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate_fn, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a2297ad-66b9-4b47-9967-47ec130cc968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  5635,  1556,  ...,     0,     0,     0],\n",
      "        [    1,    25,  1297,  ...,     0,     0,     0],\n",
      "        [    1,     0, 24347,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    1, 37793,   192,  ...,     0,     0,     0],\n",
      "        [    1,   645,  3914,  ...,     0,     0,     0],\n",
      "        [    1,   334,     6,  ...,     0,     0,     0]])\n",
      "tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
      "        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
      "        1, 0, 0, 1, 0, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "for batch in trainset:\n",
    "    print(batch['token_list'])\n",
    "    print(batch['label'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "213339c8-1f2c-405a-a76a-607974233c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 685/685 [07:42<00:00,  1.48it/s]\n",
      "100%|██████████| 76/76 [00:52<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss on epoch 1=0.46030625523659435\n",
      "Val loss on epoch 1=0.18419986767204186\n",
      "Train accuracy on epoch 1 = 0.7200159671532846\n",
      "Val accuracy on epoch 1 = 0.9293791118421053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 685/685 [07:30<00:00,  1.52it/s]\n",
      " 37%|███▋      | 28/76 [00:20<00:35,  1.34it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "epochs=5\n",
    "for epoch in range(epochs):\n",
    "    time.sleep(1)\n",
    "    total_train_loss = 0.0\n",
    "    total_val_loss = 0.0\n",
    "    total_train_acc = 0.0\n",
    "    total_val_acc = 0.0\n",
    "    model.train()\n",
    "    for i, batch in enumerate(tqdm(trainset)):\n",
    "        feature, label = batch['token_list'].to(device), batch['label'].to(device)\n",
    "#         batch_length = torch.tensor(33, dtype = torch.int64).unsqueeze(0)\n",
    "        optimizer.zero_grad()\n",
    "        output =  model(feature).squeeze()\n",
    "        loss = loss_function(output, label)\n",
    "        acc=categorical_accuracy(output,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "        total_train_acc += acc.item() \n",
    "    \n",
    "    model.eval()\n",
    "    for i, batch in enumerate(tqdm(testset)):\n",
    "        feature, label = batch['token_list'].to(device), batch['label'].to(device)\n",
    "#         batch_length = torch.tensor(33, dtype = torch.int64).unsqueeze(0)\n",
    "        optimizer.zero_grad()\n",
    "        output =  model(feature).squeeze()\n",
    "        loss = loss_function(output, label)\n",
    "        acc=categorical_accuracy(output,label)\n",
    "        total_val_loss += loss.item()\n",
    "        total_val_acc += acc.item() \n",
    "    scheduler.step()    \n",
    "    print(f\"Train loss on epoch {epoch + 1}={total_train_loss/len(trainset)}\")\n",
    "    print(f\"Val loss on epoch {epoch + 1}={total_val_loss / len(testset)}\")\n",
    "    print(f\"Train accuracy on epoch {epoch+1} = {total_train_acc/len(trainset)}\")\n",
    "    print(f\"Val accuracy on epoch {epoch+1} = {total_val_acc/len(testset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc83d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = input()\n",
    "torch.save(model.state_dict(), '{}.pth'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4806a116-90f2-4c60-bcf0-a42d5357c8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "count = 0\n",
    "gts = []\n",
    "model_preds = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(testset):\n",
    "        X, y = batch['token_list'].to(device), batch['label'].to(device)\n",
    "\n",
    "        output = model(X).squeeze()\n",
    "        acc=categorical_accuracy(output,label)\n",
    "        model_preds.extend(output.detach().cpu().numpy())\n",
    "        gts.extend(y.detach().cpu().numpy())\n",
    "        gt = y.squeeze(dim=1)\n",
    "\n",
    "        count += (model_preds == gt).sum()\n",
    "\n",
    "print(\"Accuracy on Test Data is {0:.4g}%\".format(\n",
    "    (count.item()*100)/len(testset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b64183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(gts, model_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dcfa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "auc = metrics.roc_auc_score(gts, model_preds)\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresolds = metrics.roc_curve(gts, model_preds)\n",
    "\n",
    "plt.figure(figsize=(10, 8), dpi=100)\n",
    "plt.axis('scaled')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.title(\"AUC & ROC Curve\")\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'g')\n",
    "plt.fill_between(false_positive_rate, true_positive_rate, facecolor='lightgreen', alpha=0.7)\n",
    "plt.text(0.95, 0.05, 'AUC = %0.4f' % auc, ha='right', fontsize=12, weight='bold', color='blue')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
